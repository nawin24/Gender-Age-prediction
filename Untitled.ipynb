{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b85de9-c15a-4546-a3d9-19549a77b143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 4-6 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 4-6 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 4-6 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 4-6 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 38-43 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 48-53 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 48-53 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 4-6 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 4-6 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 38-43 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 15-20 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 48-53 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 15-20 years\n",
      "Gender: Female\n",
      "Age: 38-43 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 8-12 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Male\n",
      "Age: 38-43 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n"
     ]
    }
   ],
   "source": [
    "#A Gender and Age Detection program by Mahesh Sawant\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn=frame.copy()\n",
    "    frameHeight=frameOpencvDnn.shape[0]\n",
    "    frameWidth=frameOpencvDnn.shape[1]\n",
    "    blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections=net.forward()\n",
    "    faceBoxes=[]\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence=detections[0,0,i,2]\n",
    "        if confidence>conf_threshold:\n",
    "            x1=int(detections[0,0,i,3]*frameWidth)\n",
    "            y1=int(detections[0,0,i,4]*frameHeight)\n",
    "            x2=int(detections[0,0,i,5]*frameWidth)\n",
    "            y2=int(detections[0,0,i,6]*frameHeight)\n",
    "            faceBoxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn,faceBoxes\n",
    "\n",
    "\n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument('--image')\n",
    "\n",
    "args,unknowns=parser.parse_known_args()\n",
    "\n",
    "faceProto=\"opencv_face_detector.pbtxt\"\n",
    "faceModel=\"opencv_face_detector_uint8.pb\"\n",
    "ageProto=\"age_deploy.prototxt\"\n",
    "ageModel=\"age_net.caffemodel\"\n",
    "genderProto=\"gender_deploy.prototxt\"\n",
    "genderModel=\"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList=['Male','Female']\n",
    "\n",
    "faceNet=cv2.dnn.readNet(faceModel,faceProto)\n",
    "ageNet=cv2.dnn.readNet(ageModel,ageProto)\n",
    "genderNet=cv2.dnn.readNet(genderModel,genderProto)\n",
    "\n",
    "video=cv2.VideoCapture(args.image if args.image else 0)\n",
    "padding=20\n",
    "while cv2.waitKey(1)<0 :\n",
    "    hasFrame,frame=video.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    \n",
    "    resultImg,faceBoxes=highlightFace(faceNet,frame)\n",
    "    if not faceBoxes:\n",
    "        print(\"No face detected\")\n",
    "\n",
    "    for faceBox in faceBoxes:\n",
    "        face=frame[max(0,faceBox[1]-padding):\n",
    "                   min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding)\n",
    "                   :min(faceBox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds=genderNet.forward()\n",
    "        gender=genderList[genderPreds[0].argmax()]\n",
    "        print(f'Gender: {gender}')\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds=ageNet.forward()\n",
    "        age=ageList[agePreds[0].argmax()]\n",
    "        print(f'Age: {age[1:-1]} years')\n",
    "\n",
    "        cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Detecting age and gender\", resultImg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bb8b7-dcb5-4723-a01d-a52f8a54bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 8-12 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 38-43 years\n",
      "Gender: Female\n",
      "Age: 38-43 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 38-43 years\n",
      "Gender: Male\n",
      "Age: 38-43 years\n",
      "Gender: Male\n",
      "Age: 38-43 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 38-43 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "Gender: Female\n",
      "Age: 38-43 years\n",
      "No face detected\n",
      "Gender: Male\n",
      "Age: 8-12 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Male\n",
      "Age: 25-32 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 8-12 years\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n"
     ]
    }
   ],
   "source": [
    "#A Gender and Age Detection program by Mahesh Sawant\n",
    "\n",
    "import cv2\n",
    "import math\n",
    "import argparse\n",
    "import mysql.connector\n",
    "\n",
    "mydb = mysql.connector.connect(host=\"localhost\",user=\"root\",password=\"nb24\",database=\"nbdata\")\n",
    "cursor = mydb.cursor()\n",
    "#cursor.execute('CREATE TABLE collecting_data(Gender varchar(7),Age varchar(11))')\n",
    "#cursor.execute('ALTER TABLE capturing MODIFY Age VARCHAR(10)')\n",
    "\n",
    "\n",
    "\n",
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn=frame.copy()\n",
    "    frameHeight=frameOpencvDnn.shape[0]\n",
    "    frameWidth=frameOpencvDnn.shape[1]\n",
    "    blob=cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections=net.forward()\n",
    "    faceBoxes=[]\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence=detections[0,0,i,2]\n",
    "        if confidence>conf_threshold:\n",
    "            x1=int(detections[0,0,i,3]*frameWidth)\n",
    "            y1=int(detections[0,0,i,4]*frameHeight)\n",
    "            x2=int(detections[0,0,i,5]*frameWidth)\n",
    "            y2=int(detections[0,0,i,6]*frameHeight)\n",
    "            faceBoxes.append([x1,y1,x2,y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1,y1), (x2,y2), (0,255,0), int(round(frameHeight/150)), 8)\n",
    "    return frameOpencvDnn,faceBoxes\n",
    "\n",
    "\n",
    "parser=argparse.ArgumentParser()\n",
    "parser.add_argument('--image')\n",
    "\n",
    "args,unknowns=parser.parse_known_args()\n",
    "\n",
    "faceProto=\"opencv_face_detector.pbtxt\"\n",
    "faceModel=\"opencv_face_detector_uint8.pb\"\n",
    "ageProto=\"age_deploy.prototxt\"\n",
    "ageModel=\"age_net.caffemodel\"\n",
    "genderProto=\"gender_deploy.prototxt\"\n",
    "genderModel=\"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES=(78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList=['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList=['Male','Female']\n",
    "\n",
    "faceNet=cv2.dnn.readNet(faceModel,faceProto)\n",
    "ageNet=cv2.dnn.readNet(ageModel,ageProto)\n",
    "genderNet=cv2.dnn.readNet(genderModel,genderProto)\n",
    "\n",
    "video_source= args.image  if args.image else 0\n",
    "video = cv2.VideoCapture(video_source)\n",
    "padding=20\n",
    "while cv2.waitKey(1)<0 :\n",
    "    hasFrame,frame=video.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "    \n",
    "    resultImg,faceBoxes=highlightFace(faceNet,frame)\n",
    "    if not faceBoxes:\n",
    "        print(\"No face detected\")\n",
    "\n",
    "    for faceBox in faceBoxes:\n",
    "        face=frame[max(0,faceBox[1]-padding):\n",
    "                   min(faceBox[3]+padding,frame.shape[0]-1),max(0,faceBox[0]-padding)\n",
    "                   :min(faceBox[2]+padding, frame.shape[1]-1)]\n",
    "\n",
    "        blob=cv2.dnn.blobFromImage(face, 1.0, (227,227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds=genderNet.forward()\n",
    "        global gender\n",
    "        gender=genderList[genderPreds[0].argmax()]\n",
    "        print(f'Gender: {gender}')\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds=ageNet.forward()\n",
    "        global age\n",
    "        age=ageList[agePreds[0].argmax()]\n",
    "        print(f'Age: {age[1:-1]} years')\n",
    "\n",
    "        cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBox[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Detecting age and gender\", resultImg)\n",
    "        add_data = (\"INSERT INTO collecting_data \" \n",
    "            \"(Gender, Age) \"\n",
    "            \"VALUES (%s, %s)\")\n",
    "        data = (gender, age[1:-1])\n",
    "        cursor.execute(add_data, data)\n",
    "    mydb.commit()\n",
    "cursor.close()\n",
    "mydb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7089a8d8-e793-4af3-98f2-6ff598f9ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install mysqlclient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fffb960-4d2d-4adf-856b-14495d610c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b72f6e-e6e8-43d3-a538-bede6599ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install pymysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb436c-da4a-49b5-bfeb-b0aaf6e21b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e19c02c-7fe2-4467-ae66-07c6019f666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "No face detected\n",
      "Gender: Female\n",
      "Age: 8-12 years\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported image type, must be 8bit gray or RGB image.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 98\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Get face encoding\u001b[39;00m\n\u001b[0;32m     97\u001b[0m rgb_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 98\u001b[0m face_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfaceBox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaceBox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaceBox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfaceBox\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m face_encodings:\n\u001b[0;32m    101\u001b[0m     face_encoding \u001b[38;5;241m=\u001b[39m face_encodings[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:213\u001b[0m, in \u001b[0;36mface_encodings\u001b[1;34m(face_image, known_face_locations, num_jitters, model)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mface_encodings\u001b[39m(face_image, known_face_locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, num_jitters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    Given an image, return the 128-dimension face encoding for each face in the image.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;124;03m    :return: A list of 128-dimensional face encodings (one for each face in the image)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m     raw_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_face_landmarks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mknown_face_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray(face_encoder\u001b[38;5;241m.\u001b[39mcompute_face_descriptor(face_image, raw_landmark_set, num_jitters)) \u001b[38;5;28;01mfor\u001b[39;00m raw_landmark_set \u001b[38;5;129;01min\u001b[39;00m raw_landmarks]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:165\u001b[0m, in \u001b[0;36m_raw_face_landmarks\u001b[1;34m(face_image, face_locations, model)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    163\u001b[0m     pose_predictor \u001b[38;5;241m=\u001b[39m pose_predictor_5_point\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mpose_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_location\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mface_location\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mface_locations\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\face_recognition\\api.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    163\u001b[0m     pose_predictor \u001b[38;5;241m=\u001b[39m pose_predictor_5_point\n\u001b[1;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mpose_predictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_location\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m face_location \u001b[38;5;129;01min\u001b[39;00m face_locations]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unsupported image type, must be 8bit gray or RGB image."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import math\n",
    "import argparse\n",
    "import mysql.connector\n",
    "import face_recognition\n",
    "\n",
    "# Database connection\n",
    "mydb = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"nb24\", database=\"nbdata\")\n",
    "cursor = mydb.cursor()\n",
    "\n",
    "# Function to add column if it doesn't exist\n",
    "def add_column_if_not_exists(cursor, table_name, column_name, column_definition):\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT COUNT(*) \n",
    "        FROM information_schema.COLUMNS \n",
    "        WHERE TABLE_NAME = '{table_name}' \n",
    "        AND COLUMN_NAME = '{column_name}'\n",
    "    \"\"\")\n",
    "    if cursor.fetchone()[0] == 0:\n",
    "        cursor.execute(f\"ALTER TABLE {table_name} ADD COLUMN {column_name} {column_definition}\")\n",
    "\n",
    "# Ensure the FaceCount column exists\n",
    "add_column_if_not_exists(cursor, 'collecting_data', 'FaceCount', 'INT DEFAULT 1')\n",
    "\n",
    "def highlightFace(net, frame, conf_threshold=0.7):\n",
    "    frameOpencvDnn = frame.copy()\n",
    "    frameHeight = frameOpencvDnn.shape[0]\n",
    "    frameWidth = frameOpencvDnn.shape[1]\n",
    "    blob = cv2.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n",
    "\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    faceBoxes = []\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence > conf_threshold:\n",
    "            x1 = int(detections[0, 0, i, 3] * frameWidth)\n",
    "            y1 = int(detections[0, 0, i, 4] * frameHeight)\n",
    "            x2 = int(detections[0, 0, i, 5] * frameWidth)\n",
    "            y2 = int(detections[0, 0, i, 6] * frameHeight)\n",
    "            faceBoxes.append([x1, y1, x2, y2])\n",
    "            cv2.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight / 150)), 8)\n",
    "    return frameOpencvDnn, faceBoxes\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--image')\n",
    "args, unknowns = parser.parse_known_args()\n",
    "\n",
    "faceProto = \"opencv_face_detector.pbtxt\"\n",
    "faceModel = \"opencv_face_detector_uint8.pb\"\n",
    "ageProto = \"age_deploy.prototxt\"\n",
    "ageModel = \"age_net.caffemodel\"\n",
    "genderProto = \"gender_deploy.prototxt\"\n",
    "genderModel = \"gender_net.caffemodel\"\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n",
    "genderList = ['Male', 'Female']\n",
    "\n",
    "faceNet = cv2.dnn.readNet(faceModel, faceProto)\n",
    "ageNet = cv2.dnn.readNet(ageModel, ageProto)\n",
    "genderNet = cv2.dnn.readNet(genderModel, genderProto)\n",
    "\n",
    "video_source = args.image if args.image else 0\n",
    "video = cv2.VideoCapture(video_source)\n",
    "padding = 20\n",
    "\n",
    "# List to store face encodings of detected faces\n",
    "known_face_encodings = []\n",
    "\n",
    "while cv2.waitKey(1) < 0:\n",
    "    hasFrame, frame = video.read()\n",
    "    if not hasFrame:\n",
    "        cv2.waitKey()\n",
    "        break\n",
    "\n",
    "    resultImg, faceBoxes = highlightFace(faceNet, frame)\n",
    "    if not faceBoxes:\n",
    "        print(\"No face detected\")\n",
    "\n",
    "    for faceBox in faceBoxes:\n",
    "        face = frame[max(0, faceBox[1] - padding):min(faceBox[3] + padding, frame.shape[0] - 1),\n",
    "                     max(0, faceBox[0] - padding):min(faceBox[2] + padding, frame.shape[1] - 1)]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "        genderNet.setInput(blob)\n",
    "        genderPreds = genderNet.forward()\n",
    "        gender = genderList[genderPreds[0].argmax()]\n",
    "        print(f'Gender: {gender}')\n",
    "\n",
    "        ageNet.setInput(blob)\n",
    "        agePreds = ageNet.forward()\n",
    "        age = ageList[agePreds[0].argmax()]\n",
    "        print(f'Age: {age[1:-1]} years')\n",
    "\n",
    "        # Get face encoding\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [(faceBox[1], faceBox[2], faceBox[3], faceBox[0])])\n",
    "\n",
    "        if face_encodings:\n",
    "            face_encoding = face_encodings[0]\n",
    "\n",
    "            # Check if this face is already in known_face_encodings\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.6)\n",
    "\n",
    "            if not any(matches):\n",
    "                # It's a new face\n",
    "                known_face_encodings.append(face_encoding)\n",
    "\n",
    "                # Insert the new detection into the database\n",
    "                add_data = (\"INSERT INTO collecting_data (Gender, Age, FaceCount) VALUES (%s, %s, %s)\")\n",
    "                data = (gender, age[1:-1], 1)\n",
    "                cursor.execute(add_data, data)\n",
    "            else:\n",
    "                # Existing face detected, update the FaceCount\n",
    "                update_data = (\"UPDATE collecting_data SET FaceCount = FaceCount + 1 WHERE Gender = %s AND Age = %s\")\n",
    "                data = (gender, age[1:-1])\n",
    "                cursor.execute(update_data, data)\n",
    "\n",
    "            mydb.commit()\n",
    "\n",
    "        cv2.putText(resultImg, f'{gender}, {age}', (faceBox[0], faceBox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
    "                    (0, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.imshow(\"Detecting age and gender\", resultImg)\n",
    "\n",
    "cursor.close()\n",
    "mydb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9a1f2-4ce2-40fb-97d2-fa918277079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install cmake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124409f-d8f5-4b80-ab26-82d6c24cf00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d8adc-3ef5-4348-90b7-1889fb21121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc0edf3-4bf7-4f49-8764-3072a13ae616",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install \"Desktop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35141bd3-b66c-4561-b91a-71186d181623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c2092d-8756-4bec-af65-049fee87cb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install \"C:\\Users\\user/dlib-19.24.1-cp311-cp311-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c298ee-ad14-427f-b37b-dd46645c542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install \"Desktop\\dlib-19.24.1-cp311-cp311-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3e26b-4c28-4fbc-a304-f94858c3451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install \"Desktop/dlib-19.24.1-cp311-cp311-win_amd64.whl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b56bbdd-8878-4416-aae3-7ac35e83ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad7370-ec4e-4aa4-b09f-c27a1fd4168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "def check_login():\n",
    "    username = entry_username.get()\n",
    "    password = entry_password.get()\n",
    "    \n",
    "    # Check if username and password are correct (dummy check)\n",
    "    if username == \"user\" and password == \"password\":\n",
    "        messagebox.showinfo(\"Login Successful\", \"Welcome, \" + username + \"!\")\n",
    "    else:\n",
    "        messagebox.showerror(\"Login Failed\", \"Incorrect username or password\")\n",
    "\n",
    "# Create main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Login Page\")\n",
    "\n",
    "# Create username label and entry\n",
    "label_username = tk.Label(root, text=\"Username:\")\n",
    "label_username.pack()\n",
    "entry_username = tk.Entry(root)\n",
    "entry_username.pack()\n",
    "\n",
    "# Create password label and entry\n",
    "label_password = tk.Label(root, text=\"Password:\")\n",
    "label_password.pack()\n",
    "entry_password = tk.Entry(root, show=\"*\")\n",
    "entry_password.pack()\n",
    "\n",
    "# Create login button\n",
    "btn_login = tk.Button(root, text=\"Login\", command=check_login)\n",
    "btn_login.pack()\n",
    "\n",
    "# Run the main loop\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c320cf-2c41-4929-a332-c9392cca8c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cmd\n",
    "pip install tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ab6bf-757c-4283-83ce-070a7adf1e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201ae8f-e00f-41f9-9a3f-a3f35d947abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
